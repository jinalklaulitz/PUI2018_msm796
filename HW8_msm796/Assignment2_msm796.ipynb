{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Idea: People tend to use bikes more on weekdays than weekends\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original \n",
    "##### Null Hypothesis: The proportion of people that bike during the weekends is more or the same to the proportion of people that bike on weekdays\n",
    "### $h_0$ : $\\dfrac{riders_{weekday}}{riders_{total}} \\leqslant  \\dfrac{riders_{weekend}}{riders_{total}}$\n",
    "\n",
    "##### Alternative Hypothesis: The propportion of people that bike during the weekends is less than the proportion of people that bike on weekdays \n",
    "### $h_1$ : $\\dfrac{riders_{weekday}}{riders_{total}} >\\dfrac{riders_{weekend}}{riders_{total}}$\n",
    "\n",
    "## I've modified my hypothesis to incorporate suggestions based from Sarah's comments (sj909)\n",
    "### Null Hypothesis: The average amount of bike riders during the weekends is more or the same to the average amount of bike riders that bike on weekdays\n",
    "### $h_0$ : $\\mu_{weekday} \\leqslant \\mu_{weekend}$\n",
    "\n",
    "### Alternative Hypothesis: The average amount of bike riders during the weekends is less than the average amount of bike riders that bike on weekdays\n",
    "### $h_1$ : $\\mu_{weekday} > \\mu_{weekend}$\n",
    "\n",
    "### I set signifiance level for $\\alpha = 0.05$\n",
    "\n",
    "Now i import package and other important helper functions.<br>\n",
    "be advised i used the following packages that may not be in your default environment <br>\n",
    "dateutil<br>\n",
    "requests<br>\n",
    "zipfile<br>\n",
    "io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:36:16.760777Z",
     "start_time": "2018-11-06T23:36:15.998312Z"
    }
   },
   "outputs": [],
   "source": [
    "#/home/pui_user/PUI2018_msm796/HW4_msm796\n",
    "#declare libraries\n",
    "try:\n",
    "    import urllib2 as urllib\n",
    "except ImportError:\n",
    "    import urllib.request as urllib\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta #DOWNLOAD THIS PACKAGE#\n",
    "import os\n",
    "import requests #DOWNLOAD THIS PACKAGE#\n",
    "import zipfile #DOWNLOAD THIS PACKAGE#\n",
    "import io #DOWNLOAD THIS PACKAGE#\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import subprocess as sp\n",
    "import os\n",
    "\n",
    "#library#\n",
    "    \n",
    "#declare parameters\n",
    "puidata = os.getenv(\"PUIDATA\")\n",
    "if puidata is None:\n",
    "    os.environ[\"PUIDATA\"] = \"%s/PUIdata\"%os.getenv(\"HOME\")\n",
    "    puidata = os.getenv(\"PUIDATA\")\n",
    "\n",
    "#declare helper functions\n",
    "def download_citibike_data(yearstart,yearend,monthstart,monthend,output_path):\n",
    "    '''\n",
    "    change log:\n",
    "    \n",
    "    version 1.0 - Downloads citibike datasets zip files and unzips them. does checking if file already\n",
    "    exists in path. currently has no input error handling logic.\n",
    "    ---------------------------------------------------------------------------------------------\n",
    "    Parameters:\n",
    "    name - type - description\n",
    "    yearstart - <int> - refers to the year where you want your download to start\n",
    "    yearend - <int> - refers to the year where you want your download to start\n",
    "    monthstart - <int> - refers to the month ssociated with yearstart where you want your download to start\n",
    "    monthend - <int> - refers to the month associated with yearend where you want your download to start\n",
    "    outputpath - <str> - refers to location where you want to dump the csv files\n",
    "    \n",
    "    example usage:\n",
    "    download_citibike_data(2013,2018,6,8,puidata)\n",
    "    - this downloads the citibike datasets from June 2013 to August 2018. Files will be unzipped at\n",
    "    the puidata location.\n",
    "    \n",
    "    required libraries:\n",
    "    datautil - required for year & month generation\n",
    "    requests - for downloading the zip file\n",
    "    zipfile - to unzip the file\n",
    "    io - to read binary data and give it to zipfile; effectly keeping the entire process in memory\n",
    "    '''\n",
    "    #might include in error handling in the future for incorrect input\n",
    "    curr_date = start_date=datetime.datetime(yearstart,monthstart,1).date()\n",
    "    end_date=datetime.datetime(yearend,monthend,1).date()\n",
    "    year_month=[start_date.strftime('%Y%m')]\n",
    "    base_citi_url='https://s3.amazonaws.com/tripdata/'\n",
    "    while curr_date < end_date:\n",
    "        curr_date += relativedelta(months=1)\n",
    "        year_month.append(curr_date.strftime('%Y%m'))\n",
    "    for x in year_month:\n",
    "    #need to change logic as csv filename convention isn't done properly#\n",
    "    #will just base on dates found in directory#\n",
    "        filename=x+\"-citibike-tripdata.csv\"\n",
    "        if os.path.isfile(output_path+\"/\"+filename)==True:\n",
    "            print(\"{} already exists!\".format(filename))\n",
    "        else:\n",
    "            print(\"Downloading {}\".format(filename))\n",
    "            year=int(x[:4])\n",
    "            if year>=2017:\n",
    "                ending=\".csv.zip\"\n",
    "            else:\n",
    "                ending=\".zip\"\n",
    "            url=base_citi_url+x+\"-citibike-tripdata\"+ending\n",
    "            r = requests.get(url, stream=True)\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "            z.extractall(output_path)\n",
    "            \n",
    "def append_all_citi_bike_data(input_path):\n",
    "    '''\n",
    "    developer's notes:\n",
    "    please refactor me! thanks!\n",
    "    change log:\n",
    "    \n",
    "    version 1.0 -currently has no input error handling logic or checking if file exists.\n",
    "\n",
    "    Parameters:\n",
    "    name - type - description\n",
    "    \n",
    "    input_path - <str> - location where the csv files are stored\n",
    "    -Naively appends all files into one superfile\n",
    "    \n",
    "    required libraries:\n",
    "    pandas - the output will be a dataframe, also used to load the files\n",
    "    '''\n",
    "    #currently implementing\n",
    "    files_directory=os.listdir(input_path)\n",
    "    csv_files = [x for x in files_directory if (\".csv\" in x) & ((\"citi\" in x) | (\"Citi\" in x))]\n",
    "    super_csv = pd.concat([pd.io.parsers.read_csv(input_path+\"/\"+f,engine='c',memory_map=True,low_memory='dtype') for f in csv_files],sort=False)\n",
    "#    super_csv = pd.concat([pd.read_csv(input_path+\"/\"+f,engine='c',memory_map=True,low_memory='dtype') for f in csv_files])\n",
    "    return super_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I get all the 2016 data from citibike and then concatenate them into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.667453Z",
     "start_time": "2018-11-06T23:36:16.762917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 201601-citibike-tripdata.csv\n",
      "Downloading 201602-citibike-tripdata.csv\n",
      "Downloading 201603-citibike-tripdata.csv\n",
      "Downloading 201604-citibike-tripdata.csv\n",
      "Downloading 201605-citibike-tripdata.csv\n",
      "Downloading 201606-citibike-tripdata.csv\n",
      "Downloading 201607-citibike-tripdata.csv\n",
      "Downloading 201608-citibike-tripdata.csv\n",
      "Downloading 201609-citibike-tripdata.csv\n",
      "Downloading 201610-citibike-tripdata.csv\n",
      "Downloading 201611-citibike-tripdata.csv\n",
      "Downloading 201612-citibike-tripdata.csv\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5fe37dc732f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#downloads from beginning to latest available as of late September\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdownload_citibike_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2016\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2016\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpuidata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcomplete_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mappend_all_citi_bike_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpuidata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-f8084a4eb342>\u001b[0m in \u001b[0;36mappend_all_citi_bike_data\u001b[0;34m(input_path)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0mfiles_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mcsv_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_directory\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\".csv\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"citi\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Citi\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0msuper_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'dtype'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;31m#    super_csv = pd.concat([pd.read_csv(input_path+\"/\"+f,engine='c',memory_map=True,low_memory='dtype') for f in csv_files])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper_csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    224\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                        copy=copy, sort=sort)\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mget_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    421\u001b[0m             new_data = concatenate_block_managers(\n\u001b[1;32m    422\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                 copy=self.copy)\n\u001b[0m\u001b[1;32m    424\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5420\u001b[0m             b = make_block(\n\u001b[0;32m-> 5421\u001b[0;31m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5422\u001b[0m                 placement=placement)\n\u001b[1;32m   5423\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[0;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[1;32m   5577\u001b[0m                 \u001b[0mconcat_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5578\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5579\u001b[0;31m         \u001b[0mconcat_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_concat_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5581\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/dtypes/concat.py\u001b[0m in \u001b[0;36m_concat_compat\u001b[0;34m(to_concat, axis)\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mto_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#downloads from beginning to latest available as of late September\n",
    "download_citibike_data(2016,2016,1,12,puidata)\n",
    "complete_set=append_all_citi_bike_data(puidata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.670289Z",
     "start_time": "2018-11-06T23:36:16.019Z"
    }
   },
   "outputs": [],
   "source": [
    "#complete_set.to_csv(puidata+'/complete_bike_as_of_201808.csv')\n",
    "#complete_set.to_csv(puidata+'/complete_2016_bike.csv')\n",
    "#complete_set2=pd.read_csv(puidata+'/complete_2016_bike.csv',engine='c',memory_map=True,low_memory='dtype')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Displaying the first five rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.671896Z",
     "start_time": "2018-11-06T23:36:16.035Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I generate the list of columns before i reduced the number of columns of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.673208Z",
     "start_time": "2018-11-06T23:36:16.046Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_set.columns.get_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Since the phenomenon i want to investigate only requires that information regarding days, i only keep starttime & stoptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.674954Z",
     "start_time": "2018-11-06T23:36:16.054Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols=[x for x in complete_set.columns.get_values() if x not in ['starttime','stoptime']]\n",
    "complete_set2=complete_set.drop(drop_cols,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then i display my first 10 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.676330Z",
     "start_time": "2018-11-06T23:36:16.059Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_set2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But we're curious whether or not there are more users during weekdays vs weekends, relatively speaking, so let's derive some days from our remaining columns. So first we remove rows with missing entries for our datetime fields because we cannot use them for this investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.677707Z",
     "start_time": "2018-11-06T23:36:16.066Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_set3 = complete_set2.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now derive our days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.679042Z",
     "start_time": "2018-11-06T23:36:16.073Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_set3['day_start'] = complete_set3['starttime'].apply(lambda x : datetime.datetime.strptime(x, \"%m/%d/%Y %H:%M:%S\")).apply(lambda x: x.strftime('%A'))\n",
    "complete_set3['day_end'] = complete_set3['stoptime'].apply(lambda x : datetime.datetime.strptime(x, \"%m/%d/%Y %H:%M:%S\")).apply(lambda x: x.strftime('%A'))\n",
    "#complete_set3['day_start'] = complete_set3['starttime'].astype('datetime64[ns]').apply(lambda x: x.strftime('%A'))\n",
    "#complete_set3['day_end'] = complete_set3['stoptime'].astype('datetime64[ns]').apply(lambda x: x.strftime('%A'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So now we have our days. But let's check if there are situations when some rides cross over midnight so that we can know if we can be indifferent of the variable of choice to describe our weekdays and weekends.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.680485Z",
     "start_time": "2018-11-06T23:36:16.080Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_set3.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.682373Z",
     "start_time": "2018-11-06T23:36:16.084Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_set3[complete_set3['day_start']!=complete_set3['day_end']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It appears we have a set of observations that start and end trip at different days. Since we're looking into how the day falls under a weekend or weekday affects ridership, then we should use the day when the ride begins as our basis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.684060Z",
     "start_time": "2018-11-06T23:36:16.091Z"
    }
   },
   "outputs": [],
   "source": [
    "weekend=['Saturday','Sunday']\n",
    "complete_set3['part_of_week']=complete_set3['day_start'].apply(lambda x: 'Weekend' if x in weekend else 'Weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.685581Z",
     "start_time": "2018-11-06T23:36:16.094Z"
    }
   },
   "outputs": [],
   "source": [
    "complete_set3.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's plot our counts per day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.687569Z",
     "start_time": "2018-11-06T23:36:16.100Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30,10))\n",
    "complete_set3.day_start.value_counts().plot(kind = 'bar',rot=0)\n",
    "plt.xlabel(\"Day of week\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the bar graph, we get a general sense that weekdays will tend to have more rides than weekends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of weekdays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.689111Z",
     "start_time": "2018-11-06T23:36:16.114Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "complete_set3[complete_set3['part_of_week']=='Weekday'].groupby(['day_start'])['part_of_week'].count().plot(kind='bar',rot=0)\n",
    "plt.xlabel(\"Day of week\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 2: We're dealing with a categorical distribution; a more generalized form of Bernoulli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of weekends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.691021Z",
     "start_time": "2018-11-06T23:36:16.124Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "complete_set3[complete_set3['part_of_week']=='Weekend'].groupby(['day_start'])['part_of_week'].count().plot(kind='bar',rot=0)\n",
    "plt.xlabel(\"Day of week\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3: We're dealing with a Bernoulli distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# But really, we more interested with the Distribution by part of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.693686Z",
     "start_time": "2018-11-06T23:36:16.133Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "complete_set3.groupby(['part_of_week'])['day_start'].count().plot(kind='bar',rot=0)\n",
    "plt.xlabel(\"Day of week\",fontsize=18)\n",
    "plt.ylabel(\"Count\",fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3: We're dealing with a Bernoulli distribution\n",
    "## But naturally, by comparing them with counts is inappropriate. You could come up with a misleading conclusion with the data. There are five days in a weekdays and two days in weekends. So let's get the average ridership during weekdays and weekends, then and also adjust them by normalizing them by their respective Standard Deviation to get a sense on comparing them if they were in the same scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-06T23:37:55.695676Z",
     "start_time": "2018-11-06T23:36:16.141Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_wked = complete_set3[complete_set3['part_of_week']=='Weekend'].groupby(['day_start'])['part_of_week'].count().mean()\n",
    "mean_wkdy = complete_set3[complete_set3['part_of_week']=='Weekday'].groupby(['day_start'])['part_of_week'].count().mean()\n",
    "std_wked = complete_set3[complete_set3['part_of_week']=='Weekend'].groupby(['day_start'])['part_of_week'].count().std()\n",
    "std_wkdy = complete_set3[complete_set3['part_of_week']=='Weekday'].groupby(['day_start'])['part_of_week'].count().std()\n",
    "\n",
    "print(\"sample mean ridership for weekends: {:,} ,sample mean ridership for weekdays: {:,}\".format(mean_wked,mean_wkdy))\n",
    "print(\"Standard Error ridership for weekends: {} , Standard Error ridership for weekdays: {}\".format(std_wked,std_wkdy))\n",
    "print(\"the normalized sample mean ridership for weekends: {} ,the normalized sample mean ridership for weekdays: {}\".format(mean_wked/std_wked,mean_wkdy/std_wkdy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADS",
   "language": "python",
   "name": "ads"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
